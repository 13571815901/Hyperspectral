{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains and Evaluates the IndianPines network using a feed dictionary\n",
    "========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import IndianPinesMLP \n",
    "import patch_size\n",
    "# import IndianPines_data_set as input_data\n",
    "import Spatial_dataset as input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare model parameters as external flags\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('max_steps', 20000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('hidden1', 500, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 350, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_integer('hidden3', 150, 'Number of units in hidden layer 3.')\n",
    "flags.DEFINE_integer('batch_size', 200, 'Batch size.  '\n",
    "                     'Must divide evenly into the dataset sizes.')\n",
    "# flags.DEFINE_string('train_dir', '1.mat', 'Directory to put the training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "max_steps = 20000\n",
    "IMAGE_SIZE = patch_size.patch_size\n",
    "fc1 = 500\n",
    "fc2 = 350\n",
    "fc3 = 150\n",
    "batch_size = 200\n",
    "TRAIN_FILES = 8\n",
    "TEST_FILES = 4\n",
    "DATA_PATH = os.path.join(os.getcwd(),\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed from the downloaded data in the .run() loop, below.\n",
    "    Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "    Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "    \"\"\"\n",
    "    # Note that the shapes of the placeholders match the shapes of the full\n",
    "    # image and label tensors, except the first dimension is now batch_size\n",
    "    # rather than the full size of the train or test data sets.\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IndianPinesMLP\n",
    "                                                           .IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return images_placeholder, labels_placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    \"\"\"Fills the feed_dict for training the given step.\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "    }\n",
    "    Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "    Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch size ` examples.\n",
    "    images_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\n",
    "    Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "    \"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    precision = true_count / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_DataSet(first,second):\n",
    "    temp_image = np.concatenate((first.images,second.images),axis=0)\n",
    "    temp_labels = np.concatenate((first.labels,second.labels),axis=0)\n",
    "    temp_image = temp_image.reshape(temp_image.shape[0],IMAGE_SIZE,IMAGE_SIZE,220)\n",
    "    temp_image = np.transpose(temp_image,(0,3,1,2))\n",
    "    temp_labels = np.transpose(temp_labels)\n",
    "    return input_data.DataSet(temp_image,temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    \"\"\"Train MNIST for a number of steps.\"\"\"\n",
    "    # Get the sets of images and labels for training, validation, and\n",
    "    # test on IndianPines.\n",
    "    \n",
    "    \"\"\"Concatenating all the training and test mat files\"\"\"\n",
    "    for i in range(TRAIN_FILES):\n",
    "        data_sets = input_data.read_data_sets(os.path.join(DATA_PATH, 'Train_'+str(IMAGE_SIZE)+'_'+str(i+1)+'.mat'), 'train')\n",
    "        if(i==0):\n",
    "            Training_data = data_sets\n",
    "            continue\n",
    "        else:\n",
    "            Training_data = add_DataSet(Training_data,data_sets)\n",
    "            \n",
    "    for i in range(TEST_FILES):\n",
    "        data_sets = input_data.read_data_sets(os.path.join(DATA_PATH, 'Test_'+str(IMAGE_SIZE)+'_'+str(i+1)+'.mat'),'test')\n",
    "        if(i==0):\n",
    "            Test_data = data_sets\n",
    "            continue\n",
    "        else:\n",
    "            Test_data = add_DataSet(Test_data,data_sets)\n",
    "        \n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = IndianPinesMLP.inference(images_placeholder,\n",
    "                                 FLAGS.hidden1,\n",
    "                                 FLAGS.hidden2,\n",
    "                                 FLAGS.hidden3)\n",
    "\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        loss = IndianPinesMLP.loss(logits, labels_placeholder)\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op = IndianPinesMLP.training(loss, FLAGS.learning_rate)\n",
    "\n",
    "        # Add the Op to compare the logits to the labels during evaluation.\n",
    "        eval_correct = IndianPinesMLP.evaluation(logits, labels_placeholder)\n",
    "\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "    #    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    #    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n",
    "\n",
    "        # And then after everything is built:\n",
    "\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start the training loop.\n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Fill a feed dictionary with the actual set of images and labels\n",
    "            # for this particular training step.\n",
    "            feed_dict = fill_feed_dict(Training_data,\n",
    "                                     images_placeholder,\n",
    "                                     labels_placeholder)\n",
    "\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                   feed_dict=feed_dict)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Write the summaries and print an overview fairly often.\n",
    "            if step % 50 == 0:\n",
    "            # Print status to stdout.\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "    #             summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "    #             summary_writer.add_summary(summary_str, step)\n",
    "    #             summary_writer.flush()\n",
    "\n",
    "            # Save a checkpoint and evaluate the model periodically.\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "                saver.save(sess, 'model-MLP-'+str(IMAGE_SIZE)+'X'+str(IMAGE_SIZE)+'.ckpt', global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "                print('Training Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        Training_data)\n",
    "                print('Test Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        Test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(200, 26620), dtype=float32)\n",
      "Step 0: loss = 2.77 (0.217 sec)\n",
      "Step 50: loss = 2.69 (0.046 sec)\n",
      "Step 100: loss = 2.51 (0.047 sec)\n",
      "Step 150: loss = 2.26 (0.048 sec)\n",
      "Step 200: loss = 1.98 (0.047 sec)\n",
      "Step 250: loss = 1.64 (0.045 sec)\n",
      "Step 300: loss = 1.38 (0.045 sec)\n",
      "Step 350: loss = 1.24 (0.048 sec)\n",
      "Step 400: loss = 1.22 (0.331 sec)\n",
      "Step 450: loss = 0.98 (0.044 sec)\n",
      "Step 500: loss = 0.82 (0.054 sec)\n",
      "Step 550: loss = 0.74 (0.045 sec)\n",
      "Step 600: loss = 0.73 (0.045 sec)\n",
      "Step 650: loss = 0.69 (0.045 sec)\n",
      "Step 700: loss = 0.60 (0.045 sec)\n",
      "Step 750: loss = 0.59 (0.045 sec)\n",
      "Step 800: loss = 0.57 (0.317 sec)\n",
      "Step 850: loss = 0.51 (0.043 sec)\n",
      "Step 900: loss = 0.47 (0.046 sec)\n",
      "Step 950: loss = 0.55 (0.040 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 2817  Precision @ 1: 0.8803\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1034  Precision @ 1: 0.6462\n",
      "Step 1000: loss = 0.45 (0.041 sec)\n",
      "Step 1050: loss = 0.44 (0.051 sec)\n",
      "Step 1100: loss = 0.32 (0.044 sec)\n",
      "Step 1150: loss = 0.35 (0.050 sec)\n",
      "Step 1200: loss = 0.35 (0.330 sec)\n",
      "Step 1250: loss = 0.29 (0.048 sec)\n",
      "Step 1300: loss = 0.24 (0.048 sec)\n",
      "Step 1350: loss = 0.25 (0.042 sec)\n",
      "Step 1400: loss = 0.23 (0.044 sec)\n",
      "Step 1450: loss = 0.28 (0.044 sec)\n",
      "Step 1500: loss = 0.24 (0.045 sec)\n",
      "Step 1550: loss = 0.19 (0.039 sec)\n",
      "Step 1600: loss = 0.18 (0.322 sec)\n",
      "Step 1650: loss = 0.28 (0.044 sec)\n",
      "Step 1700: loss = 0.19 (0.045 sec)\n",
      "Step 1750: loss = 0.20 (0.045 sec)\n",
      "Step 1800: loss = 0.17 (0.044 sec)\n",
      "Step 1850: loss = 0.21 (0.044 sec)\n",
      "Step 1900: loss = 0.23 (0.044 sec)\n",
      "Step 1950: loss = 0.18 (0.045 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3089  Precision @ 1: 0.9653\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1297  Precision @ 1: 0.8106\n",
      "Step 2000: loss = 0.13 (0.346 sec)\n",
      "Step 2050: loss = 0.13 (0.045 sec)\n",
      "Step 2100: loss = 0.14 (0.045 sec)\n",
      "Step 2150: loss = 0.11 (0.042 sec)\n",
      "Step 2200: loss = 0.10 (0.048 sec)\n",
      "Step 2250: loss = 0.09 (0.045 sec)\n",
      "Step 2300: loss = 0.09 (0.044 sec)\n",
      "Step 2350: loss = 0.11 (0.045 sec)\n",
      "Step 2400: loss = 0.04 (0.327 sec)\n",
      "Step 2450: loss = 0.07 (0.050 sec)\n",
      "Step 2500: loss = 0.08 (0.044 sec)\n",
      "Step 2550: loss = 0.08 (0.049 sec)\n",
      "Step 2600: loss = 0.07 (0.046 sec)\n",
      "Step 2650: loss = 0.04 (0.044 sec)\n",
      "Step 2700: loss = 0.05 (0.045 sec)\n",
      "Step 2750: loss = 0.05 (0.045 sec)\n",
      "Step 2800: loss = 0.04 (0.329 sec)\n",
      "Step 2850: loss = 0.04 (0.042 sec)\n",
      "Step 2900: loss = 0.04 (0.045 sec)\n",
      "Step 2950: loss = 0.03 (0.044 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3189  Precision @ 1: 0.9966\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1377  Precision @ 1: 0.8606\n",
      "Step 3000: loss = 0.04 (0.039 sec)\n",
      "Step 3050: loss = 0.04 (0.044 sec)\n",
      "Step 3100: loss = 0.04 (0.046 sec)\n",
      "Step 3150: loss = 0.03 (0.045 sec)\n",
      "Step 3200: loss = 0.03 (0.339 sec)\n",
      "Step 3250: loss = 0.04 (0.050 sec)\n",
      "Step 3300: loss = 0.02 (0.046 sec)\n",
      "Step 3350: loss = 0.02 (0.040 sec)\n",
      "Step 3400: loss = 0.03 (0.044 sec)\n",
      "Step 3450: loss = 0.02 (0.044 sec)\n",
      "Step 3500: loss = 0.03 (0.046 sec)\n",
      "Step 3550: loss = 0.02 (0.041 sec)\n",
      "Step 3600: loss = 0.02 (0.304 sec)\n",
      "Step 3650: loss = 0.02 (0.043 sec)\n",
      "Step 3700: loss = 0.02 (0.043 sec)\n",
      "Step 3750: loss = 0.02 (0.040 sec)\n",
      "Step 3800: loss = 0.02 (0.043 sec)\n",
      "Step 3850: loss = 0.03 (0.043 sec)\n",
      "Step 3900: loss = 0.02 (0.044 sec)\n",
      "Step 3950: loss = 0.01 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1411  Precision @ 1: 0.8819\n",
      "Step 4000: loss = 0.01 (0.323 sec)\n",
      "Step 4050: loss = 0.01 (0.044 sec)\n",
      "Step 4100: loss = 0.01 (0.046 sec)\n",
      "Step 4150: loss = 0.01 (0.044 sec)\n",
      "Step 4200: loss = 0.01 (0.043 sec)\n",
      "Step 4250: loss = 0.01 (0.045 sec)\n",
      "Step 4300: loss = 0.01 (0.045 sec)\n",
      "Step 4350: loss = 0.01 (0.043 sec)\n",
      "Step 4400: loss = 0.01 (0.330 sec)\n",
      "Step 4450: loss = 0.01 (0.043 sec)\n",
      "Step 4500: loss = 0.01 (0.039 sec)\n",
      "Step 4550: loss = 0.01 (0.048 sec)\n",
      "Step 4600: loss = 0.01 (0.042 sec)\n",
      "Step 4650: loss = 0.01 (0.041 sec)\n",
      "Step 4700: loss = 0.01 (0.043 sec)\n",
      "Step 4750: loss = 0.01 (0.044 sec)\n",
      "Step 4800: loss = 0.01 (0.304 sec)\n",
      "Step 4850: loss = 0.01 (0.043 sec)\n",
      "Step 4900: loss = 0.01 (0.050 sec)\n",
      "Step 4950: loss = 0.01 (0.040 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1418  Precision @ 1: 0.8862\n",
      "Step 5000: loss = 0.01 (0.054 sec)\n",
      "Step 5050: loss = 0.01 (0.044 sec)\n",
      "Step 5100: loss = 0.01 (0.043 sec)\n",
      "Step 5150: loss = 0.01 (0.044 sec)\n",
      "Step 5200: loss = 0.01 (0.295 sec)\n",
      "Step 5250: loss = 0.01 (0.040 sec)\n",
      "Step 5300: loss = 0.01 (0.041 sec)\n",
      "Step 5350: loss = 0.01 (0.045 sec)\n",
      "Step 5400: loss = 0.01 (0.042 sec)\n",
      "Step 5450: loss = 0.01 (0.043 sec)\n",
      "Step 5500: loss = 0.01 (0.043 sec)\n",
      "Step 5550: loss = 0.01 (0.043 sec)\n",
      "Step 5600: loss = 0.01 (0.304 sec)\n",
      "Step 5650: loss = 0.01 (0.049 sec)\n",
      "Step 5700: loss = 0.01 (0.043 sec)\n",
      "Step 5750: loss = 0.01 (0.043 sec)\n",
      "Step 5800: loss = 0.00 (0.041 sec)\n",
      "Step 5850: loss = 0.01 (0.042 sec)\n",
      "Step 5900: loss = 0.00 (0.045 sec)\n",
      "Step 5950: loss = 0.01 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1423  Precision @ 1: 0.8894\n",
      "Step 6000: loss = 0.01 (0.311 sec)\n",
      "Step 6050: loss = 0.00 (0.044 sec)\n",
      "Step 6100: loss = 0.01 (0.043 sec)\n",
      "Step 6150: loss = 0.00 (0.043 sec)\n",
      "Step 6200: loss = 0.01 (0.044 sec)\n",
      "Step 6250: loss = 0.01 (0.043 sec)\n",
      "Step 6300: loss = 0.00 (0.043 sec)\n",
      "Step 6350: loss = 0.00 (0.048 sec)\n",
      "Step 6400: loss = 0.00 (0.298 sec)\n",
      "Step 6450: loss = 0.00 (0.044 sec)\n",
      "Step 6500: loss = 0.00 (0.041 sec)\n",
      "Step 6550: loss = 0.00 (0.038 sec)\n",
      "Step 6600: loss = 0.00 (0.044 sec)\n",
      "Step 6650: loss = 0.00 (0.050 sec)\n",
      "Step 6700: loss = 0.00 (0.044 sec)\n",
      "Step 6750: loss = 0.00 (0.043 sec)\n",
      "Step 6800: loss = 0.00 (0.316 sec)\n",
      "Step 6850: loss = 0.00 (0.044 sec)\n",
      "Step 6900: loss = 0.00 (0.045 sec)\n",
      "Step 6950: loss = 0.00 (0.047 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1426  Precision @ 1: 0.8912\n",
      "Step 7000: loss = 0.00 (0.057 sec)\n",
      "Step 7050: loss = 0.00 (0.041 sec)\n",
      "Step 7100: loss = 0.00 (0.043 sec)\n",
      "Step 7150: loss = 0.00 (0.046 sec)\n",
      "Step 7200: loss = 0.00 (0.304 sec)\n",
      "Step 7250: loss = 0.00 (0.043 sec)\n",
      "Step 7300: loss = 0.00 (0.043 sec)\n",
      "Step 7350: loss = 0.00 (0.043 sec)\n",
      "Step 7400: loss = 0.00 (0.043 sec)\n",
      "Step 7450: loss = 0.00 (0.044 sec)\n",
      "Step 7500: loss = 0.00 (0.043 sec)\n",
      "Step 7550: loss = 0.00 (0.043 sec)\n",
      "Step 7600: loss = 0.00 (0.304 sec)\n",
      "Step 7650: loss = 0.00 (0.043 sec)\n",
      "Step 7700: loss = 0.00 (0.049 sec)\n",
      "Step 7750: loss = 0.00 (0.042 sec)\n",
      "Step 7800: loss = 0.00 (0.043 sec)\n",
      "Step 7850: loss = 0.00 (0.043 sec)\n",
      "Step 7900: loss = 0.00 (0.043 sec)\n",
      "Step 7950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1427  Precision @ 1: 0.8919\n",
      "Step 8000: loss = 0.00 (0.314 sec)\n",
      "Step 8050: loss = 0.00 (0.043 sec)\n",
      "Step 8100: loss = 0.00 (0.039 sec)\n",
      "Step 8150: loss = 0.00 (0.036 sec)\n",
      "Step 8200: loss = 0.00 (0.043 sec)\n",
      "Step 8250: loss = 0.00 (0.044 sec)\n",
      "Step 8300: loss = 0.00 (0.043 sec)\n",
      "Step 8350: loss = 0.00 (0.045 sec)\n",
      "Step 8400: loss = 0.00 (0.303 sec)\n",
      "Step 8450: loss = 0.00 (0.043 sec)\n",
      "Step 8500: loss = 0.00 (0.043 sec)\n",
      "Step 8550: loss = 0.00 (0.043 sec)\n",
      "Step 8600: loss = 0.00 (0.043 sec)\n",
      "Step 8650: loss = 0.00 (0.043 sec)\n",
      "Step 8700: loss = 0.00 (0.043 sec)\n",
      "Step 8750: loss = 0.00 (0.044 sec)\n",
      "Step 8800: loss = 0.00 (0.303 sec)\n",
      "Step 8850: loss = 0.00 (0.043 sec)\n",
      "Step 8900: loss = 0.00 (0.044 sec)\n",
      "Step 8950: loss = 0.00 (0.045 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1429  Precision @ 1: 0.8931\n",
      "Step 9000: loss = 0.00 (0.043 sec)\n",
      "Step 9050: loss = 0.00 (0.039 sec)\n",
      "Step 9100: loss = 0.00 (0.043 sec)\n",
      "Step 9150: loss = 0.00 (0.043 sec)\n",
      "Step 9200: loss = 0.00 (0.304 sec)\n",
      "Step 9250: loss = 0.00 (0.044 sec)\n",
      "Step 9300: loss = 0.00 (0.044 sec)\n",
      "Step 9350: loss = 0.00 (0.044 sec)\n",
      "Step 9400: loss = 0.00 (0.043 sec)\n",
      "Step 9450: loss = 0.00 (0.043 sec)\n",
      "Step 9500: loss = 0.00 (0.043 sec)\n",
      "Step 9550: loss = 0.00 (0.043 sec)\n",
      "Step 9600: loss = 0.00 (0.303 sec)\n",
      "Step 9650: loss = 0.00 (0.043 sec)\n",
      "Step 9700: loss = 0.00 (0.043 sec)\n",
      "Step 9750: loss = 0.00 (0.043 sec)\n",
      "Step 9800: loss = 0.00 (0.043 sec)\n",
      "Step 9850: loss = 0.00 (0.043 sec)\n",
      "Step 9900: loss = 0.00 (0.043 sec)\n",
      "Step 9950: loss = 0.00 (0.046 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1431  Precision @ 1: 0.8944\n",
      "Step 10000: loss = 0.00 (0.305 sec)\n",
      "Step 10050: loss = 0.00 (0.043 sec)\n",
      "Step 10100: loss = 0.00 (0.043 sec)\n",
      "Step 10150: loss = 0.00 (0.042 sec)\n",
      "Step 10200: loss = 0.00 (0.043 sec)\n",
      "Step 10250: loss = 0.00 (0.043 sec)\n",
      "Step 10300: loss = 0.00 (0.043 sec)\n",
      "Step 10350: loss = 0.00 (0.043 sec)\n",
      "Step 10400: loss = 0.00 (0.302 sec)\n",
      "Step 10450: loss = 0.00 (0.043 sec)\n",
      "Step 10500: loss = 0.00 (0.040 sec)\n",
      "Step 10550: loss = 0.00 (0.043 sec)\n",
      "Step 10600: loss = 0.00 (0.043 sec)\n",
      "Step 10650: loss = 0.00 (0.043 sec)\n",
      "Step 10700: loss = 0.00 (0.043 sec)\n",
      "Step 10750: loss = 0.00 (0.043 sec)\n",
      "Step 10800: loss = 0.00 (0.303 sec)\n",
      "Step 10850: loss = 0.00 (0.043 sec)\n",
      "Step 10900: loss = 0.00 (0.043 sec)\n",
      "Step 10950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1431  Precision @ 1: 0.8944\n",
      "Step 11000: loss = 0.00 (0.046 sec)\n",
      "Step 11050: loss = 0.00 (0.045 sec)\n",
      "Step 11100: loss = 0.00 (0.043 sec)\n",
      "Step 11150: loss = 0.00 (0.044 sec)\n",
      "Step 11200: loss = 0.00 (0.304 sec)\n",
      "Step 11250: loss = 0.00 (0.043 sec)\n",
      "Step 11300: loss = 0.00 (0.044 sec)\n",
      "Step 11350: loss = 0.00 (0.044 sec)\n",
      "Step 11400: loss = 0.00 (0.043 sec)\n",
      "Step 11450: loss = 0.00 (0.044 sec)\n",
      "Step 11500: loss = 0.00 (0.043 sec)\n",
      "Step 11550: loss = 0.00 (0.043 sec)\n",
      "Step 11600: loss = 0.00 (0.285 sec)\n",
      "Step 11650: loss = 0.00 (0.043 sec)\n",
      "Step 11700: loss = 0.00 (0.043 sec)\n",
      "Step 11750: loss = 0.00 (0.043 sec)\n",
      "Step 11800: loss = 0.00 (0.043 sec)\n",
      "Step 11850: loss = 0.00 (0.043 sec)\n",
      "Step 11900: loss = 0.00 (0.043 sec)\n",
      "Step 11950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1432  Precision @ 1: 0.8950\n",
      "Step 12000: loss = 0.00 (0.305 sec)\n",
      "Step 12050: loss = 0.00 (0.043 sec)\n",
      "Step 12100: loss = 0.00 (0.043 sec)\n",
      "Step 12150: loss = 0.00 (0.043 sec)\n",
      "Step 12200: loss = 0.00 (0.043 sec)\n",
      "Step 12250: loss = 0.00 (0.043 sec)\n",
      "Step 12300: loss = 0.00 (0.043 sec)\n",
      "Step 12350: loss = 0.00 (0.043 sec)\n",
      "Step 12400: loss = 0.00 (0.304 sec)\n",
      "Step 12450: loss = 0.00 (0.043 sec)\n",
      "Step 12500: loss = 0.00 (0.043 sec)\n",
      "Step 12550: loss = 0.00 (0.043 sec)\n",
      "Step 12600: loss = 0.00 (0.044 sec)\n",
      "Step 12650: loss = 0.00 (0.043 sec)\n",
      "Step 12700: loss = 0.00 (0.043 sec)\n",
      "Step 12750: loss = 0.00 (0.043 sec)\n",
      "Step 12800: loss = 0.00 (0.303 sec)\n",
      "Step 12850: loss = 0.00 (0.044 sec)\n",
      "Step 12900: loss = 0.00 (0.043 sec)\n",
      "Step 12950: loss = 0.00 (0.041 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1432  Precision @ 1: 0.8950\n",
      "Step 13000: loss = 0.00 (0.048 sec)\n",
      "Step 13050: loss = 0.00 (0.043 sec)\n",
      "Step 13100: loss = 0.00 (0.043 sec)\n",
      "Step 13150: loss = 0.00 (0.043 sec)\n",
      "Step 13200: loss = 0.00 (0.305 sec)\n",
      "Step 13250: loss = 0.00 (0.049 sec)\n",
      "Step 13300: loss = 0.00 (0.043 sec)\n",
      "Step 13350: loss = 0.00 (0.049 sec)\n",
      "Step 13400: loss = 0.00 (0.045 sec)\n",
      "Step 13450: loss = 0.00 (0.042 sec)\n",
      "Step 13500: loss = 0.00 (0.043 sec)\n",
      "Step 13550: loss = 0.00 (0.043 sec)\n",
      "Step 13600: loss = 0.00 (0.303 sec)\n",
      "Step 13650: loss = 0.00 (0.043 sec)\n",
      "Step 13700: loss = 0.00 (0.047 sec)\n",
      "Step 13750: loss = 0.00 (0.043 sec)\n",
      "Step 13800: loss = 0.00 (0.045 sec)\n",
      "Step 13850: loss = 0.00 (0.043 sec)\n",
      "Step 13900: loss = 0.00 (0.042 sec)\n",
      "Step 13950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1433  Precision @ 1: 0.8956\n",
      "Step 14000: loss = 0.00 (0.312 sec)\n",
      "Step 14050: loss = 0.00 (0.044 sec)\n",
      "Step 14100: loss = 0.00 (0.043 sec)\n",
      "Step 14150: loss = 0.00 (0.043 sec)\n",
      "Step 14200: loss = 0.00 (0.043 sec)\n",
      "Step 14250: loss = 0.00 (0.043 sec)\n",
      "Step 14300: loss = 0.00 (0.043 sec)\n",
      "Step 14350: loss = 0.00 (0.043 sec)\n",
      "Step 14400: loss = 0.00 (0.300 sec)\n",
      "Step 14450: loss = 0.00 (0.043 sec)\n",
      "Step 14500: loss = 0.00 (0.042 sec)\n",
      "Step 14550: loss = 0.00 (0.043 sec)\n",
      "Step 14600: loss = 0.00 (0.043 sec)\n",
      "Step 14650: loss = 0.00 (0.050 sec)\n",
      "Step 14700: loss = 0.00 (0.043 sec)\n",
      "Step 14750: loss = 0.00 (0.043 sec)\n",
      "Step 14800: loss = 0.00 (0.338 sec)\n",
      "Step 14850: loss = 0.00 (0.046 sec)\n",
      "Step 14900: loss = 0.00 (0.044 sec)\n",
      "Step 14950: loss = 0.00 (0.044 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1433  Precision @ 1: 0.8956\n",
      "Step 15000: loss = 0.00 (0.053 sec)\n",
      "Step 15050: loss = 0.00 (0.043 sec)\n",
      "Step 15100: loss = 0.00 (0.043 sec)\n",
      "Step 15150: loss = 0.00 (0.043 sec)\n",
      "Step 15200: loss = 0.00 (0.304 sec)\n",
      "Step 15250: loss = 0.00 (0.043 sec)\n",
      "Step 15300: loss = 0.00 (0.043 sec)\n",
      "Step 15350: loss = 0.00 (0.043 sec)\n",
      "Step 15400: loss = 0.00 (0.043 sec)\n",
      "Step 15450: loss = 0.00 (0.044 sec)\n",
      "Step 15500: loss = 0.00 (0.043 sec)\n",
      "Step 15550: loss = 0.00 (0.043 sec)\n",
      "Step 15600: loss = 0.00 (0.303 sec)\n",
      "Step 15650: loss = 0.00 (0.043 sec)\n",
      "Step 15700: loss = 0.00 (0.044 sec)\n",
      "Step 15750: loss = 0.00 (0.043 sec)\n",
      "Step 15800: loss = 0.00 (0.043 sec)\n",
      "Step 15850: loss = 0.00 (0.048 sec)\n",
      "Step 15900: loss = 0.00 (0.043 sec)\n",
      "Step 15950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1433  Precision @ 1: 0.8956\n",
      "Step 16000: loss = 0.00 (0.315 sec)\n",
      "Step 16050: loss = 0.00 (0.043 sec)\n",
      "Step 16100: loss = 0.00 (0.044 sec)\n",
      "Step 16150: loss = 0.00 (0.042 sec)\n",
      "Step 16200: loss = 0.00 (0.042 sec)\n",
      "Step 16250: loss = 0.00 (0.042 sec)\n",
      "Step 16300: loss = 0.00 (0.043 sec)\n",
      "Step 16350: loss = 0.00 (0.045 sec)\n",
      "Step 16400: loss = 0.00 (0.304 sec)\n",
      "Step 16450: loss = 0.00 (0.042 sec)\n",
      "Step 16500: loss = 0.00 (0.041 sec)\n",
      "Step 16550: loss = 0.00 (0.044 sec)\n",
      "Step 16600: loss = 0.00 (0.043 sec)\n",
      "Step 16650: loss = 0.00 (0.043 sec)\n",
      "Step 16700: loss = 0.00 (0.043 sec)\n",
      "Step 16750: loss = 0.00 (0.043 sec)\n",
      "Step 16800: loss = 0.00 (0.334 sec)\n",
      "Step 16850: loss = 0.00 (0.045 sec)\n",
      "Step 16900: loss = 0.00 (0.043 sec)\n",
      "Step 16950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1434  Precision @ 1: 0.8962\n",
      "Step 17000: loss = 0.00 (0.045 sec)\n",
      "Step 17050: loss = 0.00 (0.043 sec)\n",
      "Step 17100: loss = 0.00 (0.043 sec)\n",
      "Step 17150: loss = 0.00 (0.043 sec)\n",
      "Step 17200: loss = 0.00 (0.303 sec)\n",
      "Step 17250: loss = 0.00 (0.049 sec)\n",
      "Step 17300: loss = 0.00 (0.043 sec)\n",
      "Step 17350: loss = 0.00 (0.043 sec)\n",
      "Step 17400: loss = 0.00 (0.043 sec)\n",
      "Step 17450: loss = 0.00 (0.043 sec)\n",
      "Step 17500: loss = 0.00 (0.043 sec)\n",
      "Step 17550: loss = 0.00 (0.048 sec)\n",
      "Step 17600: loss = 0.00 (0.297 sec)\n",
      "Step 17650: loss = 0.00 (0.040 sec)\n",
      "Step 17700: loss = 0.00 (0.043 sec)\n",
      "Step 17750: loss = 0.00 (0.043 sec)\n",
      "Step 17800: loss = 0.00 (0.043 sec)\n",
      "Step 17850: loss = 0.00 (0.043 sec)\n",
      "Step 17900: loss = 0.00 (0.044 sec)\n",
      "Step 17950: loss = 0.00 (0.050 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1434  Precision @ 1: 0.8962\n",
      "Step 18000: loss = 0.00 (0.309 sec)\n",
      "Step 18050: loss = 0.00 (0.043 sec)\n",
      "Step 18100: loss = 0.00 (0.043 sec)\n",
      "Step 18150: loss = 0.00 (0.043 sec)\n",
      "Step 18200: loss = 0.00 (0.043 sec)\n",
      "Step 18250: loss = 0.00 (0.043 sec)\n",
      "Step 18300: loss = 0.00 (0.043 sec)\n",
      "Step 18350: loss = 0.00 (0.044 sec)\n",
      "Step 18400: loss = 0.00 (0.304 sec)\n",
      "Step 18450: loss = 0.00 (0.043 sec)\n",
      "Step 18500: loss = 0.00 (0.043 sec)\n",
      "Step 18550: loss = 0.00 (0.043 sec)\n",
      "Step 18600: loss = 0.00 (0.046 sec)\n",
      "Step 18650: loss = 0.00 (0.043 sec)\n",
      "Step 18700: loss = 0.00 (0.038 sec)\n",
      "Step 18750: loss = 0.00 (0.043 sec)\n",
      "Step 18800: loss = 0.00 (0.304 sec)\n",
      "Step 18850: loss = 0.00 (0.044 sec)\n",
      "Step 18900: loss = 0.00 (0.043 sec)\n",
      "Step 18950: loss = 0.00 (0.044 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1434  Precision @ 1: 0.8962\n",
      "Step 19000: loss = 0.00 (0.047 sec)\n",
      "Step 19050: loss = 0.00 (0.043 sec)\n",
      "Step 19100: loss = 0.00 (0.043 sec)\n",
      "Step 19150: loss = 0.00 (0.045 sec)\n",
      "Step 19200: loss = 0.00 (0.304 sec)\n",
      "Step 19250: loss = 0.00 (0.036 sec)\n",
      "Step 19300: loss = 0.00 (0.044 sec)\n",
      "Step 19350: loss = 0.00 (0.044 sec)\n",
      "Step 19400: loss = 0.00 (0.043 sec)\n",
      "Step 19450: loss = 0.00 (0.041 sec)\n",
      "Step 19500: loss = 0.00 (0.043 sec)\n",
      "Step 19550: loss = 0.00 (0.044 sec)\n",
      "Step 19600: loss = 0.00 (0.303 sec)\n",
      "Step 19650: loss = 0.00 (0.043 sec)\n",
      "Step 19700: loss = 0.00 (0.045 sec)\n",
      "Step 19750: loss = 0.00 (0.043 sec)\n",
      "Step 19800: loss = 0.00 (0.043 sec)\n",
      "Step 19850: loss = 0.00 (0.043 sec)\n",
      "Step 19900: loss = 0.00 (0.043 sec)\n",
      "Step 19950: loss = 0.00 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 3200  Num correct: 3200  Precision @ 1: 1.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 1600  Num correct: 1434  Precision @ 1: 0.8962\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
