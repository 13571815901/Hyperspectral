<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Hyperspectral by KGPML</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Hyperspectral</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/KGPML/Hyperspectral" class="btn">View on GitHub</a>
      <a href="https://github.com/KGPML/Hyperspectral/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/KGPML/Hyperspectral/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="deep-learning-for-land-cover-classification-in-hyperspectral-images" class="anchor" href="#deep-learning-for-land-cover-classification-in-hyperspectral-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deep Learning for Land-cover Classification in Hyperspectral Images</h1>

<p>Hyperspectral images are images captured in multiple bands of the electromagnetic spectrum. This project is focussed at the development of Deep Learned Artificial Neural Networks for robust landcover classification in hyperspectral images. Land-cover classification is the task of assigning to every pixel, a class label that represents the type of land-cover present in the location of the pixel. It is an image segmentation/scenelabeling task. The following diagram describes the task.</p>

<hr>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/landcover-classification.png?raw=True" width="600"> </p>

<hr>

<p>This website describes our explorations with the performance of Multi-Layer Perceptrons and Convolutional Neural Networks at the task of Land-cover Classification in Hyperspectral Images.</p>

<hr>

<h1>
<a id="dataset" class="anchor" href="#dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset</h1>

<p>We have performed our experiments on the <a href="https://purr.purdue.edu/publications/1947/1">Indian Pines Dataset</a>. The following are the particulars of the dataset: </p>

<ul>
<li>Source: AVIRIS sensor</li>
<li>Region: Indian Pines test site over north-western Indiana</li>
<li>Time of the year: June</li>
<li>Wavelength range: 0.4 â€“ 2.5 micron</li>
<li>Number of spectral bands: 220</li>
<li>Size of image: 145x145 pixel</li>
<li>Number of land-cover classes: 16</li>
</ul>

<hr>

<h1>
<a id="hardware-used" class="anchor" href="#hardware-used" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hardware used</h1>

<p>The neural networks were trained on a machine with dual Intel Xeon E5-2630 v2 CPUs, 32 GB RAM and NVIDIA Tesla K-20C GPU. </p>

<hr>

<h1>
<a id="multi-layer-perceptron" class="anchor" href="#multi-layer-perceptron" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Multi-Layer Perceptron</h1>

<p>Multi-Layer Perceptron (MLP) is an artificial neural network with one or more hidden layers of neurons. MLP is capable of modelling highly non-linear functions between the input and output and forms the basis of Deep-learning Neural Network (DNN) models.</p>

<h2>
<a id="architecture-of-multi-layer-perceptron-used" class="anchor" href="#architecture-of-multi-layer-perceptron-used" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Architecture of Multi-Layer Perceptron used</h2>

<p><strong>input- [affine - relu] x 3 - affine - softmax</strong></p>

<p>(Schematic representation below)</p>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/architecture-MLP.png?raw=True" width="400"></p>

<p><code>N</code>denotes the size of the input patch.</p>

<hr>

<h2>
<a id="specifics-of-the-learning-algorithm" class="anchor" href="#specifics-of-the-learning-algorithm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Specifics of the learning algorithm</h2>

<p>The following are the details of the learning algorithm used:</p>

<ul>
<li>
<p>Parameter update algorithm used: <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adagrad</a></p>

<ul>
<li>Batch size: 200</li>
<li>Learning rate: 0.01</li>
</ul>
</li>
<li><p>Number of steps: until best validation performance</p></li>
</ul>

<hr>

<h2>
<a id="performance" class="anchor" href="#performance" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance</h2>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/accuracy-bar-MLP.png?raw=True" width="500"> </p>

<p>Decoding generated for different input patch sizes:</p>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/performance-MLP.png?raw=True" width="800"> </p>

<hr>

<h1>
<a id="convolutional-neural-network" class="anchor" href="#convolutional-neural-network" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Convolutional Neural Network</h1>

<p>(CNN or ConvNet) are a special category of artificial neural networks designed for processing data with a gridlike structure. The ConvNet architecture is based on sparse interactions and parameter sharing and is highly effective for efficient learning of spatial invariances in images. There are four kinds of layers in a typical ConvNet architecture: convolutional (conv), pooling (pool), fullyconnected (affine) and rectifying linear unit (ReLU). Each convolutional layer transforms one set of feature maps into another set of feature maps by convolution with a set of filters.</p>

<h2>
<a id="architecture-of-convolutional-neural-network-used" class="anchor" href="#architecture-of-convolutional-neural-network-used" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Architecture of Convolutional Neural Network used</h2>

<p><strong>input- [conv - relu - maxpool] x 2 - [affine - relu] x 2 - affine - softmax</strong></p>

<p>(Schematic representation below)</p>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/architecture-CNN.png?raw=True" width="400"></p>

<p><code>N</code>denotes the size of the input patch.</p>

<hr>

<h2>
<a id="specifics-of-the-learning-algorithm-1" class="anchor" href="#specifics-of-the-learning-algorithm-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Specifics of the learning algorithm</h2>

<p>The following are the details of the learning algorithm used:</p>

<ul>
<li>
<p>Parameter update algorithm used: Mini-batch gradient descent</p>

<ul>
<li>Batch size: 100</li>
<li>Learning rate: 0.01</li>
</ul>
</li>
<li><p>Number of steps: until best validation performance</p></li>
</ul>

<hr>

<h2>
<a id="performance-1" class="anchor" href="#performance-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance</h2>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/accuracy-bar-CNN.png?raw=True" width="500"> </p>

<p>Decoding generated for different input patch sizes:</p>

<p><img src="https://github.com/KGPML/Hyperspectral/blob/master/images/performance-CNN.jpg?raw=True" width="800"> </p>

<hr>

<hr>

<h1>
<a id="description-of-the-repository" class="anchor" href="#description-of-the-repository" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Description of the repository</h1>

<ul>
<li>
<p><code>IndianPines_DataSet_Preparation_Without_Augmentation.ipynb</code> does the following operations:</p>

<ul>
<li>Loads the Indian Pines dataset</li>
<li>Scales the input between [0,1]</li>
<li>Mean normalizes the channels</li>
<li>Makes training and test splits</li>
<li>Extracts patches of given size</li>
<li>Oversamples the training set for balancing the classes</li>
</ul>
</li>
<li><p><code>Spatial_dataset.py</code> provides a highly flexible Dataset class for handling the Indian Pines data.</p></li>
<li><p><code>patch_size.py</code> specify the required patch-size here.</p></li>
<li>
<p><code>IndianPinesCNN.ipynb</code> builds the TensorFlow Convolutional Neural Network and defines the training and evaluation ops:</p>

<ul>
<li>inference() - Builds the model as far as is required for running the network forward to make predictions.</li>
<li>loss() - Adds to the inference model the layers required to generate loss.</li>
<li>training() - Adds to the loss model the Ops required to generate and apply gradients.</li>
<li>evaluation() - Calcuates the classification accuracy </li>
</ul>
</li>
<li><p><code>CNN_feed.ipynb</code> Trains and evaluates the Neural Network using a feed dictionary</p></li>
<li><p><code>Decoder_Spatial_CNN.ipynb</code> Generates the landcover classification of an input hyperspectral image for a given trained network</p></li>
<li>
<p><code>IndianPinesMLP.py</code> builds the TensorFlow Multi-layer Perceptron and defines the training and evaluation ops:</p>

<ul>
<li>inference() - Builds the model as far as is required for running the network forward to make predictions.</li>
<li>loss() - Adds to the inference model the layers required to generate loss.</li>
<li>training() - Adds to the loss model the Ops required to generate and apply gradients.</li>
<li>evaluation() - Calcuates the classification accuracy </li>
</ul>
</li>
<li><p><code>MLP_feed.ipynb</code> Trains and evaluates the MLP using a feed dictionary</p></li>
<li><p><code>Decoder_Spatial_MLP.ipynb</code> Generates the landcover classification of an input hyperspectral image for a given trained network</p></li>
<li><p><code>credibility.ipynb</code> Summarizes the predictions of an ensemble and produces the land-cover classification and class-wise confusion matrix.</p></li>
</ul>

<hr>

<h2>
<a id="setting-up-the-experiment-" class="anchor" href="#setting-up-the-experiment-" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setting up the experiment </h2>

<ul>
<li>Download the Indian Pines data-set from <a href="https://purr.purdue.edu/publications/1947/1">here</a>.</li>
<li>Make a directory named <code>Data</code> within the current working directory and copy the downloaded .mat files <code>Indian_pines.mat</code> and <code>Indian_pines_gt.mat</code> in this directory.</li>
</ul>

<p>In order to make sure all codes run smoothly, you should have the following directory subtree structure under your current working directory:</p>

<pre><code>|-- IndianPines_DataSet_Preparation_Without_Augmentation.ipynb
|-- Decoder_Spatial_CNN.ipynb
|-- Decoder_Spatial_MLP.ipynb
|-- IndianPinesCNN.ipynb
|-- CNN_feed.ipynb
|-- MLP_feed.ipynb
|-- credibility.ipynb
|-- IndianPinesCNN.py
|-- IndianPinesMLP.py
|-- Spatial_dataset.py
|-- patch_size.py
|-- Data
|   |-- Indian_pines_gt.mat
|   |-- Indian_pines.mat


</code></pre>

<ul>
<li>Set the required patch-size value (eg. 11, 21, etc) in <code>patch_size.py</code> and run the following notebooks in order:

<ol>
<li><code>IndianPines_DataSet_Preparation_Without_Augmentation.ipynb</code></li>
<li>
<code>CNN_feed.ipynb</code> OR <code>MLP_feed.ipynb</code> (specify the number of fragments in the training and test data in the variables <code>TRAIN_FILES</code> and <code>TEST_FILES</code>)</li>
<li>
<code>Decoder_Spatial_CNN.ipynb</code> OR <code>Decoder_Spatial_MLP.ipynb</code> (set the required checkpoint to be used for decoding in the <code>model_name</code> variable)</li>
</ol>
</li>
</ul>

<p>Outputs will be displayed in the notebooks.</p>

<hr>

<h2>
<a id="acknowledgement" class="anchor" href="#acknowledgement" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgement</h2>

<p>This repository was developed by <a href="http://santara.github.io">Anirban Santara</a>, <a href="https://in.linkedin.com/in/ankit-singh-78252394">Ankit Singh</a>, <a href="https://www.linkedin.com/in/pranoot-hatwar-27553579">Pranoot Hatwar</a> and <a href="https://www.linkedin.com/in/kaustubh-mani-48363584">Kaustubh Mani</a> under the supervision of <a href="http://cse.iitkgp.ac.in/%7Epabitra/">Prof. Pabitra Mitra</a> during June-July, 2016 at the Deprtment of Computer Science and Engineering, Indian Institute of Technology Kharagpur, India. The project is funded by <a href="http://www.sac.gov.in/">Satellite Applications Centre, Indian Space Research Organization (SAC-ISRO)</a>.  </p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/KGPML/Hyperspectral">Hyperspectral</a> is maintained by <a href="https://github.com/KGPML">KGPML</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
